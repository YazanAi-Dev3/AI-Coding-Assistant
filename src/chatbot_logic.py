# src/chatbot_logic.py (Final Simplified Version)

import joblib
import numpy as np
import logging
from sklearn.metrics.pairwise import cosine_similarity
from model_loader import load_embedding_model, load_generative_model

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)-8s - %(message)s')

class HybridChatbot:
    def __init__(self, db_path='database/vector_database.pkl'):
        logging.info("Initializing the Hybrid Chatbot...")
        self.db_path = db_path
        self.vector_db = self._load_vector_db()
        
        # Load the ready-to-use embedding model
        self.embedding_model = load_embedding_model()
        
        # Load the generative model and its tokenizer
        self.generative_model, self.generative_tokenizer = load_generative_model()
        
        if not all([self.vector_db, self.embedding_model, self.generative_model, self.generative_tokenizer]):
            raise RuntimeError("Failed to initialize one or more components.")
            
        logging.info("Hybrid Chatbot initialized successfully.")

    def _load_vector_db(self):
        try:
            with open(self.db_path, 'rb') as f:
                db = joblib.load(f)
            logging.info(f"Vector database loaded from '{self.db_path}'")
            return db
        except FileNotFoundError:
            logging.error(f"Vector database not found at '{self.db_path}'. Please run 'build_vector_db.py' first.")
            return None

    def get_answer(self, user_question, similarity_threshold=0.75):
        """
        Gets an answer using the simplified .encode() method.
        """
        logging.info(f"Received new question: '{user_question}'")
        
        # Use the simple .encode() method from the SentenceTransformer model
        question_embedding = self.embedding_model.encode([user_question])
        
        similarities = cosine_similarity(question_embedding, self.vector_db['embeddings'])
        most_similar_idx = np.argmax(similarities)
        max_similarity = similarities[0, most_similar_idx]
        
        logging.info(f"Highest similarity score found: {max_similarity:.4f}")
        
        if max_similarity >= similarity_threshold:
            answer = self.vector_db['answers'][most_similar_idx]
            source = "Retrieved from Database (High Confidence)"
            logging.info("Retrieving stored answer.")
        else:
            logging.info("Generating a new answer with Phi-2.")
            inputs = self.generative_tokenizer(user_question, return_tensors="pt", return_attention_mask=False)
            outputs = self.generative_model.generate(**inputs, max_length=200)
            answer = self.generative_tokenizer.decode(outputs[0])
            source = "Generated by Phi-2 (Low Confidence Match)"
            
        return answer, source